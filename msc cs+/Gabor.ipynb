{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T23:37:35.723141Z",
     "start_time": "2023-07-25T23:37:35.322615900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decomposition(object):\n",
    "    def mean_pooling(self, img, size):\n",
    "        \"\"\"\n",
    "        :param img:\n",
    "        :param size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n, m = img.shape\n",
    "        fn, fm = int(n / size), int(m / size)\n",
    "        fimg = np.zeros((fn, fm), dtype=float)\n",
    "        for i in range(fn):\n",
    "            for j in range(fm):\n",
    "                sum = 0\n",
    "                for x in range(i * size, i * size + size):\n",
    "                    for y in range(j * size, j * size + size):\n",
    "                        sum += img[x, y]\n",
    "                fimg[i, j] = sum / (size * size)\n",
    "        return fimg\n",
    "\n",
    "    def PCA(self, train, test, n=1000):\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        pca = PCA(n_components=n)\n",
    "        train = pca.fit_transform(train)\n",
    "        test = pca.transform(test)\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def knn(self, train_array, test_array):\n",
    "        \"\"\"\n",
    "        :param train_array:\n",
    "        :param test_array:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 存储训练样本的最大值\n",
    "        train_array_max = []\n",
    "        train_array_min = []\n",
    "\n",
    "        # 分别记录训练样本的数量及特征量数目+1\n",
    "        n = train_array.shape[0]\n",
    "        m = train_array.shape[1]\n",
    "\n",
    "        # 测试样本的数目\n",
    "        test_n = test_array.shape[0]\n",
    "\n",
    "        # 提取训练样本和测试样本的特征量和真实结果\n",
    "        train_x = train_array[:, :m - 1].reshape(n, m - 1)\n",
    "        train_y = train_array[:, m - 1].reshape(n, )\n",
    "        test_x = test_array[:, :m - 1].reshape(test_n, m - 1)\n",
    "        test_y = test_array[:, m - 1].reshape(test_n, )\n",
    "\n",
    "        # 将特征量归一化\n",
    "        for i in range(m - 1):\n",
    "            train_array_max.append(np.max(train_x[:, i]))\n",
    "            train_array_min.append(np.min(train_x[:, i]))\n",
    "            if (train_array_max[i] - train_array_min[i]) != 0:\n",
    "                train_x[:, i] = (train_x[:, i] - train_array_min[i]) / (train_array_max[i] - train_array_min[i])\n",
    "                test_x[:, i] = (test_x[:, i] - train_array_min[i]) / (train_array_max[i] - train_array_min[i])\n",
    "\n",
    "        # 利用最邻近算法进行预测训练数据结果\n",
    "        result = []\n",
    "        for x1 in test_x:\n",
    "            distance = []\n",
    "            for x2 in train_x:\n",
    "                distance.append(np.sum((x1 - x2) * (x1 - x2)))\n",
    "            result.append(train_y[distance.index(min(distance))])\n",
    "\n",
    "        # 获得识别率\n",
    "        recognition_rate = np.sum((result == test_y)) / len(test_y)\n",
    "        return recognition_rate, np.array(result).astype('int')\n",
    "\n",
    "    def SVM(self, train_data, test_data):\n",
    "        \"\"\"\n",
    "        :param train_data:\n",
    "        :param test_data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        from sklearn.svm import SVC\n",
    "        svc = SVC(kernel='rbf', gamma='scale')\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        x_train, y_train = train_data[:, :train_data.shape[1] - 1], train_data[:, -1]\n",
    "        x_test, y_test = test_data[:, :test_data.shape[1] - 1], test_data[:, -1]\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        new_train, new_test = Decomposition().PCA(x_train, x_test,\n",
    "                                                  min(1000, min(x_train.shape[1] - 7, x_train.shape[0])))\n",
    "\n",
    "        svc.fit(new_train, y_train)\n",
    "        pred = svc.predict(new_test)\n",
    "\n",
    "        # 获得识别率\n",
    "        recognition_rate = np.sum((pred == y_test)) / len(test_data[:, -1])\n",
    "        print(recognition_rate)\n",
    "\n",
    "    def MLP(self, train_data, test_data):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        x_train, y_train = train_data[:, :train_data.shape[1] - 1], train_data[:, -1]\n",
    "        x_test, y_test = test_data[:, :test_data.shape[1] - 1], test_data[:, -1]\n",
    "\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        hiden_num = int((train_data.shape[1] + 7) * 2 / 3)\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                            hidden_layer_sizes=(hiden_num, hiden_num, hiden_num, hiden_num, hiden_num, hiden_num),\n",
    "                            random_state=1)\n",
    "        mlp.fit(x_train, y_train)\n",
    "        pred = mlp.predict(x_test)\n",
    "\n",
    "        # 获得识别率\n",
    "        recognition_rate = np.sum((pred == y_test)) / len(test_data[:, -1])\n",
    "        print(recognition_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gabor(object):\n",
    "\n",
    "    def build_filters(self):\n",
    "        \"\"\"\n",
    "        构建Gabor滤波器\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        filters = []\n",
    "        ksize = [3, 5, 7, 9]  # gabor尺度，6个\n",
    "        lamda = np.pi / 2.0  # 波长\n",
    "        for theta in np.arange(0, np.pi, np.pi / 4):  # gabor方向，0°，45°，90°，135°，共四个\n",
    "            for K in range(4):\n",
    "                kern = cv2.getGaborKernel((ksize[K], ksize[K]), 0.56 * ksize[K], theta, lamda, 0.5, 1, ktype=cv2.CV_32F)\n",
    "                kern /= 1.5 * kern.sum()\n",
    "                filters.append(kern)\n",
    "        return filters\n",
    "\n",
    "    def process(self, img, filters):\n",
    "        accum = np.zeros_like(img)\n",
    "        for kern in filters:\n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "            np.maximum(accum, fimg, accum)\n",
    "        return accum\n",
    "\n",
    "    def getGabor(self, img, filters, pic_show=False, reduction=1):\n",
    "        res = []  # 滤波结果\n",
    "        for i in range(len(filters)):\n",
    "            res1 = self.process(img, filters[i])\n",
    "            res1 = Decomposition().mean_pooling(res1, reduction)\n",
    "            # print(res1.shape)\n",
    "            res.append(np.asarray(res1))\n",
    "        if pic_show:\n",
    "            plt.figure(2)\n",
    "            for temp in range(len(filters)):\n",
    "                plt.subplot(4, 4, temp + 1)\n",
    "                plt.imshow(filters[temp], cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "        return res  # 返回滤波结果,结果为24幅图，按照gabor角度排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gabor(object):\n",
    "\n",
    "    def build_filters(self):\n",
    "        \"\"\"\n",
    "        构建Gabor滤波器\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        filters = []\n",
    "        ksize = [3, 5, 7, 9]  # gabor尺度，6个\n",
    "        lamda = np.pi / 2.0  # 波长\n",
    "        for theta in np.arange(0, np.pi, np.pi / 4):  # gabor方向，0°，45°，90°，135°，共四个\n",
    "            for K in range(4):\n",
    "                kern = cv2.getGaborKernel((ksize[K], ksize[K]), 0.56 * ksize[K], theta, lamda, 0.5, 1, ktype=cv2.CV_32F)\n",
    "                kern /= 1.5 * kern.sum()\n",
    "                filters.append(kern)\n",
    "        return filters\n",
    "\n",
    "    def process(self, img, filters):\n",
    "        accum = np.zeros_like(img)\n",
    "        for kern in filters:\n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "            np.maximum(accum, fimg, accum)\n",
    "        return accum\n",
    "\n",
    "    def getGabor(self, img, filters, pic_show=False, reduction=1):\n",
    "        res = []  # 滤波结果\n",
    "        for i in range(len(filters)):\n",
    "            res1 = self.process(img, filters[i])\n",
    "            res1 = Decomposition().mean_pooling(res1, reduction)\n",
    "            # print(res1.shape)\n",
    "            res.append(np.asarray(res1))\n",
    "        if pic_show:\n",
    "            plt.figure(2)\n",
    "            for temp in range(len(filters)):\n",
    "                plt.subplot(4, 4, temp + 1)\n",
    "                plt.imshow(filters[temp], cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "        return res  # 返回滤波结果,结果为24幅图，按照gabor角度排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_valid(data_op=1, op=1, reduction=1, rate=0.2):\n",
    "    \"\"\"\n",
    "    评估函数，在三个数据集上进行评估\n",
    "    :param op: 1-all 2-part\n",
    "    :param data_op: 1-CK 2-Fer 3-Jaffe\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import preprocess\n",
    "    from tqdm import tqdm\n",
    "    from data import CK, Fer2013, Jaffe\n",
    "\n",
    "    filters = Gabor().build_filters()\n",
    "    if data_op == 1:\n",
    "        _, x, y = CK().gen_train_no()\n",
    "    if data_op == 2:\n",
    "        _, x, y = Fer2013().gen_train_no()\n",
    "    if data_op == 3:\n",
    "        _, x, y = Jaffe().gen_train_no()\n",
    "    train = []\n",
    "    if op == 1:\n",
    "        for i in tqdm(np.arange(0, x.shape[0], 1)):\n",
    "            x[i] = preprocess.gray_norm(x[i])\n",
    "            x[i] = preprocess.adaptive_histogram_equalization(x[i])\n",
    "            res = Gabor().getGabor(x[i], filters, False, reduction)\n",
    "            res = np.array(res).reshape(-1)\n",
    "            res = np.append(res, y[i])\n",
    "            train.append(res)\n",
    "        train = np.array(train)\n",
    "\n",
    "    if data_op != 2:\n",
    "        # 需要划分\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(train, train, random_state=2019, test_size=rate)\n",
    "        Classifier().SVM(x_train, x_test)\n",
    "    test1 = []\n",
    "    test2 = []\n",
    "    if data_op == 2:\n",
    "        _, x, y = Fer2013().gen_valid_no(1)\n",
    "        for i in tqdm(np.arange(0, x.shape[0], 1)):\n",
    "            x[i] = preprocess.gray_norm(x[i])\n",
    "            x[i] = preprocess.adaptive_histogram_equalization(x[i])\n",
    "            res = Gabor().getGabor(x[i], filters, False, reduction)\n",
    "            res = np.array(res).reshape(-1)\n",
    "            res = np.append(res, y[i])\n",
    "            test1.append(res)\n",
    "\n",
    "        _, x, y = Fer2013().gen_valid_no(2)\n",
    "        for i in tqdm(np.arange(0, x.shape[0], 1)):\n",
    "            x[i] = preprocess.gray_norm(x[i])\n",
    "            x[i] = preprocess.adaptive_histogram_equalization(x[i])\n",
    "            res = Gabor().getGabor(x[i], filters, False, reduction)\n",
    "            res = np.array(res).reshape(-1)\n",
    "            res = np.append(res, y[i])\n",
    "            test2.append(res)\n",
    "        test1 = np.array(test1)\n",
    "        test2 = np.array(test2)\n",
    "        print(\"Public\")\n",
    "        Classifier().SVM(train, test1)\n",
    "        print(\"Pirvate\")\n",
    "        Classifier().SVM(train, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test():\n",
    "    \"\"\"\n",
    "    在未训练的数据集上进行测试\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    filters = Gabor().build_filters()\n",
    "    from tqdm import tqdm\n",
    "    from data import CK, Fer2013, Jaffe\n",
    "    _, x, y = Fer2013().gen_train_no()\n",
    "    train = []\n",
    "    for i in tqdm(np.arange(0, x.shape[0], 1)):\n",
    "        x[i] = preprocess.gray_norm(x[i])\n",
    "        x[i] = preprocess.adaptive_histogram_equalization(x[i])\n",
    "        res = Gabor().getGabor(x[i], filters, False, 6)\n",
    "        res = np.array(res).reshape(-1)\n",
    "        res = np.append(res, y[i])\n",
    "        train.append(res)\n",
    "    train = np.array(train)\n",
    "\n",
    "    test = []\n",
    "    _, x, y = Jaffe().gen_train_no()\n",
    "    for i in tqdm(np.arange(0, x.shape[0], 1)):\n",
    "        x[i] = preprocess.gray_norm(x[i])\n",
    "        x[i] = preprocess.adaptive_histogram_equalization(x[i])\n",
    "        res = Gabor().getGabor(x[i], filters, False, 6)\n",
    "        res = np.array(res).reshape(-1)\n",
    "        res = np.append(res, y[i])\n",
    "        test.append(res)\n",
    "    test = np.array(train)\n",
    "\n",
    "    Classifier().SVM(train, test)\n",
    "\n",
    "    test = []\n",
    "    _, x, y = CK().gen_train_no()\n",
    "    for i in tqdm(np.arange(0, x.shape[0], 1)):\n",
    "        x[i] = preprocess.gray_norm(x[i])\n",
    "        x[i] = preprocess.adaptive_histogram_equalization(x[i])\n",
    "        res = Gabor().getGabor(x[i], filters, False, 6)\n",
    "        res = np.array(res).reshape(-1)\n",
    "        res = np.append(res, y[i])\n",
    "        test.append(res)\n",
    "    test = np.array(train)\n",
    "    Classifier().SVM(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 在本数据集上训练并评估\n",
    "    # 0.9645 0.949 (784, 36865) (197, 36865) re = 6\n",
    "    print(\"CK+:\")\n",
    "    evaluate_valid(1, 1, 3)\n",
    "    # public: 0.458 private : 0.389\n",
    "    # print(\"Fer20 13:\")\n",
    "    # evaluate_valid(2, 1, 8)\n",
    "    # # 0.4186 0.697\n",
    "    # print(\"Jaffe\")\n",
    "    # evaluate_valid(3, 2, 1, 0.1)\n",
    "    # # 在不同数据集上训练并评估\n",
    "    # # Jaffe 0.705 CK: 0.705\n",
    "    # evaluate_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
