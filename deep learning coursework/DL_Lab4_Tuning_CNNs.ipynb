{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL Lab4 - Tuning CNNs",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3ZWUOMwa1dV"
      },
      "source": [
        "# Lab 4 - Tune the hyperparameters of a CNN on MNIST\n",
        "\n",
        "This tutorial walks through using Ax to tune two hyperparameters (learning rate and momentum) for a PyTorch CNN on the MNIST dataset trained using SGD with momentum. Adapted from https://ax.dev/tutorials/tune_cnn.html and a tutorial about the methods is available at https://ax.dev/docs/bayesopt.html\n",
        "\n",
        "1. Run through the tutorial, then \n",
        "2. Write your own code for grid search and random search (remember the logarithmic transforms).\n",
        "3. Create your own `net` structure to go into the `train_evaluate()` function and try optimising your own network. Consider also adapting the network structure. You could also try adapting some of the examples used in earlier labs to see whether you can improve on your results with hyperparameter search.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXVtBN-NbQoO"
      },
      "source": [
        "!pip3 install ax-platform "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxDNGv2la1dX"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from ax.plot.contour import plot_contour\n",
        "from ax.plot.trace import optimization_trace_single_method\n",
        "from ax.service.managed_loop import optimize\n",
        "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
        "from ax.utils.tutorials.cnn_utils import load_mnist, train, evaluate, CNN\n",
        "\n",
        "init_notebook_plotting(offline=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWv-BG38a1dd"
      },
      "source": [
        "torch.manual_seed(12345)\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2BEchUja1dg"
      },
      "source": [
        "## 1. Load MNIST data\n",
        "First, we need to load the MNIST data and partition it into training, validation, and test sets.\n",
        "\n",
        "Note: this will download the dataset if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_n1IOP5a1dh",
        "scrolled": true
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "train_loader, valid_loader, test_loader = load_mnist(batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6HTi-Hua1dl"
      },
      "source": [
        "## 2. Define function to optimize\n",
        "In this tutorial, we want to optimize classification accuracy on the validation set as a function of the learning rate and momentum. The function takes in a parameterization (set of parameter values), computes the classification accuracy, and returns a dictionary of metric name ('accuracy') to a tuple with the mean and standard error.\n",
        "\n",
        "The CNN() function is a simple preconfigured ConvNet - see the code at https://ax.dev/api/_modules/ax/utils/tutorials/cnn_utils.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVHNWvCda1dm"
      },
      "source": [
        "print(CNN())\n",
        "\n",
        "def train_evaluate(parameterization):\n",
        "    net = CNN()\n",
        "    net = train(net=net, train_loader=train_loader, parameters=parameterization, dtype=dtype, device=device)\n",
        "    return evaluate(\n",
        "        net=net,\n",
        "        data_loader=valid_loader,\n",
        "        dtype=dtype,\n",
        "        device=device,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmjK2pofa1dq"
      },
      "source": [
        "## 3. Run the optimization loop\n",
        "Here, we set the bounds on the learning rate and momentum and set the parameter space for the learning rate to be on a log scale. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvJJOuLTa1dr"
      },
      "source": [
        "#ax.optimize(parameters, evaluation_function, experiment_name=None, objective_name=None, minimize=False, parameter_constraints=None, outcome_constraints=None, total_trials=20, arms_per_trial=1, random_seed=None, generation_strategy=None)\n",
        "\n",
        "best_parameters, values, experiment, model = optimize(\n",
        "    parameters=[\n",
        "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 0.4], \"log_scale\": True},\n",
        "        {\"name\": \"momentum\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
        "    ],\n",
        "    evaluation_function=train_evaluate,\n",
        "    objective_name='accuracy',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1NnIghVa1dv"
      },
      "source": [
        "We can introspect the optimal parameters and their outcomes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pD-MfzGa1dw"
      },
      "source": [
        "best_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3cavNPaa1dz"
      },
      "source": [
        "means, covariances = values\n",
        "means, covariances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkb2Fsxza1d2"
      },
      "source": [
        "## 4. Plot response surface\n",
        "\n",
        "Contour plot showing classification accuracy as a function of the two hyperparameters.\n",
        "\n",
        "The black squares show points that we have actually run, notice how they are clustered in the optimal region."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY3UQMqZhEoy"
      },
      "source": [
        "# some boilerplate to make things render on colab\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'colab'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nWRy1cTa1d3"
      },
      "source": [
        "render(plot_contour(model=model, param_x='lr', param_y='momentum', metric_name='accuracy'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jGpPHM0a1d6"
      },
      "source": [
        "## 5. Plot best objective as function of the iteration\n",
        "\n",
        "Show the model accuracy improving as we identify better hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZaWoB_-a1d7"
      },
      "source": [
        "# `plot_single_method` expects a 2-d array of means, because it expects to average means from multiple \n",
        "# optimization runs, so we wrap out best objectives array in another array.\n",
        "best_objectives = np.array([[trial.objective_mean*100 for trial in experiment.trials.values()]])\n",
        "best_objective_plot = optimization_trace_single_method(\n",
        "    y=np.maximum.accumulate(best_objectives, axis=1),\n",
        "    title=\"Model performance vs. # of iterations\",\n",
        "    ylabel=\"Classification Accuracy, %\",\n",
        ")\n",
        "render(best_objective_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiUZz1l1a1d-"
      },
      "source": [
        "## 6. Train CNN with best hyperparameters and evaluate on test set\n",
        "Note that the resulting accuracy on the test set might not be exactly the same as the maximum accuracy achieved on the evaluation set throughout optimization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1SNWQaDa1d_"
      },
      "source": [
        "data = experiment.fetch_data()\n",
        "df = data.df\n",
        "best_arm_name = df.arm_name[df['mean'] == df['mean'].max()].values[0]\n",
        "best_arm = experiment.arms_by_name[best_arm_name]\n",
        "best_arm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp4rYaQda1eG"
      },
      "source": [
        "combined_train_valid_set = torch.utils.data.ConcatDataset([\n",
        "    train_loader.dataset.dataset, \n",
        "    valid_loader.dataset.dataset,\n",
        "])\n",
        "combined_train_valid_loader = torch.utils.data.DataLoader(\n",
        "    combined_train_valid_set, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "541FzVBza1eM"
      },
      "source": [
        "net = train(\n",
        "    net=CNN(),\n",
        "    train_loader=combined_train_valid_loader, \n",
        "    parameters=best_arm.parameters,\n",
        "    dtype=dtype,\n",
        "    device=device,\n",
        ")\n",
        "test_accuracy = evaluate(\n",
        "    net=net,\n",
        "    data_loader=test_loader,\n",
        "    dtype=dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Blb0eCa1e7"
      },
      "source": [
        "print(f\"Classification Accuracy (test set): {round(test_accuracy*100, 2)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwU5EkzGyOHB"
      },
      "source": [
        "# Task - apply this approach to a new problem\n",
        "You can apply this approach to any of the lab tasks we have tried earlier, or you can optimise other parameters than in this example.\n",
        "\n",
        "For example, you can try adding $\\ell_1$ or $\\ell_2$ regularisation to the model.  The $\\ell_2$ case is straightforward -- just pass a parameter `\"weight_decay\"` for the $\\ell_2$ parameter to the `parameters=[` code above.\n",
        "\n",
        "\n",
        "\n",
        "You will also find these hyperparameter optimisation tools useful in your Assessed Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ2EvEzjZPo_"
      },
      "source": [
        "If you want to go a bit further you could try experimenting with $\\ell_1$ regularisation.  Testing $\\ell_1$ regularisation is slightly more tricky. One approach would be to adapt the `train()` command in https://ax.dev/api/_modules/ax/utils/tutorials/cnn_utils.html then augmenting your training loop with the additional terms for the loss function associated with the $\\ell_1$ cost.\n",
        "\n",
        "l1_penalty = nn.L1Loss(size_average=False)\n",
        "\n",
        "reg_loss = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "\n",
        ">  reg_loss += l1_penalty(param)\n",
        "\n",
        ">  factor = const_val #lambda\n",
        " \n",
        ">  loss += factor * reg_loss \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW4-Aea-ZW1l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}