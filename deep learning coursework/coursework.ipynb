{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function, division, unicode_literals\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from functools import partial\n",
    "from PIL import Image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation before working"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.show the number of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_number():\n",
    "    picture_frames = pd.read_csv('train.csv')\n",
    "    labels = picture_frames.iloc[:,1]\n",
    "    labels = labels.to_list()\n",
    "    x = set(labels)\n",
    "    y = []\n",
    "    for a in x:\n",
    "        y.append(labels.count(a))\n",
    "    x = list(x)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.bar(x, y, facecolor='#1f77b4', edgecolor='k')\n",
    "    # plt.xticks(rotation=90)\n",
    "    plt.tick_params(labelsize=15)\n",
    "    plt.xlabel('class', fontsize=10)\n",
    "    plt.ylabel('number of data', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use_cuda = True\n",
    "# device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "# device\n",
    "# torch.cuda.device_count()\n",
    "# train_dir = os.path.abspath(\"train\")\n",
    "# img_name = os.path.join(train_dir,\"1.png\")\n",
    "# img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# picture_frames = pd.read_csv('train.csv')\n",
    "# picture_name = picture_frames.iloc[:,0]\n",
    "# label = picture_frames.iloc[:,1]\n",
    "# picture_name = np.asarray(picture_name)\n",
    "# label = np.asarray(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CellPictureDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, csv_file=None, transform=None, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        if csv_file is not None:\n",
    "            self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = os.path.abspath(root_dir)\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if self.test is False:\n",
    "            img_name = os.path.join(self.root_dir,\n",
    "                                    self.data_frame.iloc[idx, 0])\n",
    "        else:\n",
    "            img_name = os.path.join(self.root_dir, os.listdir(self.root_dir)[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            self.image = self.transform(image)\n",
    "        if self.test is False:\n",
    "            label = self.data_frame.iloc[idx, 1]\n",
    "            # label = np.asarray(label)\n",
    "            # self.label = torch.from_numpy(label).unsqueeze_(0)\n",
    "            # sample = [self.image, self.label]\n",
    "            return self.image, label\n",
    "        else:\n",
    "            return self.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "class MyConvNet(nn.Module):\n",
    "    def __init__(self, l1=120):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(collections.OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3,32,5,padding=2)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(2,2)),\n",
    "            ('conv2', nn.Conv2d(32,64,5,padding=2)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('pool1', nn.MaxPool2d(2,2)),\n",
    "            # Put in a linear layers ...\n",
    "            ('flatten', nn.Flatten()),                                          \n",
    "            ('fc1', nn.Linear(802816,l1)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(l1,4)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyConvNet()\n",
    "data = torch.ones(10,3,224,224)\n",
    "result = model(data)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "# train_dataset = CellPictureDataset('train', csv_file='train.csv', transform=data_transform)\n",
    "# test_dataset = CellPictureDataset('train',transform=data_transform, test=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.visualize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "#                                       transforms.RandomHorizontalFlip(),\n",
    "#                                       transforms.ToTensor(),\n",
    "#                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#                                      ])\n",
    "# train_dataset = CellPictureDataset('train', csv_file='train.csv', transform=train_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def show_a_batch():\n",
    "    images, labels = next(iter(train_loader))\n",
    "\n",
    "    images = images.numpy()\n",
    "\n",
    "    n=6\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    grid = ImageGrid(fig, 111,  # 类似绘制子图 subplot(111)\n",
    "                    nrows_ncols=(n, n),  # 创建 n 行 m 列的 axes 网格\n",
    "                    axes_pad=0.02,  # 网格间距\n",
    "                    share_all=True\n",
    "                    )\n",
    "\n",
    "    # 遍历每张图像\n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.imshow(im.transpose((1,2,0)))\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = MyConvNet()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# learning_rate = 1e-2\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def training_loop():\n",
    "#     total_train_step = 0\n",
    "#     total_test_step = 0\n",
    "#     epoch = 10\n",
    "#     for i in range(epoch):\n",
    "#         print(\"-------no.{} train begin\".format(i+1))\n",
    "#         for data in train_loader:\n",
    "#             image, label = data\n",
    "#             outputs = model(image)\n",
    "#             loss = loss_fn(outputs, label)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_train_step = total_train_step + 1\n",
    "#             if total_train_step % 100 == 0:\n",
    "#                 print(\"trian times: {}, Loss: {}\".format(total_train_step, loss.item()))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def load_data(train_dir=\"train/\", test_dir = \"test/\"):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     ])\n",
    "\n",
    "#     train_dataset = CellPictureDataset(train_dir, csv_file='train.csv', transform=transform)\n",
    "#     test_dataset = CellPictureDataset(test_dir,transform=transform, test=True)\n",
    "\n",
    "#     return train_dataset, test_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Function of training in tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
    "    net = MyConvNet(config[\"l1\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # trainset, testset = load_data(data_dir)\n",
    "    csv_dir = \"D://python_code//deep learning coursework//train.csv\"\n",
    "    # data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "    train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "    train_dir = \"D://python_code//deep learning coursework//train\"\n",
    "    trainset = CellPictureDataset(train_dir, csv_file=csv_dir, transform=train_transform)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader =DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=0)\n",
    "    valloader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=0)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            # if i % 100 == 99:  # print every 2000 mini-batches\n",
    "            #     print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "            #                                     running_loss / epoch_steps))\n",
    "            #     running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "#     print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#         \"l1\": 120,\n",
    "#         \"lr\": 1e-2,\n",
    "#         \"batch_size\": 4,\n",
    "#         \"momentum\": 0.9\n",
    "#     }\n",
    "# train_cifar(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.code for calculate the accurary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    root_dir = \"D://python_code//deep learning coursework//train\"\n",
    "    csv_dir = \"D://python_code//deep learning coursework//train.csv\"\n",
    "    data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "    trainset = CellPictureDataset(root_dir=root_dir, csv_file=csv_dir, transform=data_transform)\n",
    "    testloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    \n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Code for training the model usin tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    # train_dir = os.path.abspath(\"train\")\n",
    "    # load_data(train_dir)\n",
    "    configs = {\n",
    "        # \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(6, 10)),\n",
    "        \"l1\": 256,\n",
    "        \"lr\": tune.loguniform(5e-3, 1e-2),\n",
    "        # \"batch_size\": tune.choice([16, 32]),\n",
    "        \"batch_size\": 16,\n",
    "        \"momentum\": tune.loguniform(6e-1, 8e-1)\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        train_cifar,\n",
    "        resources_per_trial={\"gpu\": gpus_per_trial},\n",
    "        config=configs,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = MyConvNet(best_trial.config[\"l1\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.dir_or_data\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main(num_samples=15, max_num_epochs=10, gpus_per_trial=1)\n",
    "    # config = {\n",
    "    #     \"l1\": 120,\n",
    "    #     \"l2\": 84,\n",
    "    #     \"lr\": 1e-2,\n",
    "    #     \"batch_size\": 4,\n",
    "    #     \"momentum\": 0.9\n",
    "    # }\n",
    "    # train_cifar(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best trial config: {'l1': 256, 'lr': 0.008830556690188944, 'batch_size': 16, 'momentum': 0.6994160916279658}\n",
    "Best trial final validation loss: 0.3116818554699421\n",
    "Best trial final validation accuracy: 0.8888888888888888"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for drawing the loss and accurary curves when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_visualize(model, learning_rate, monmentum):\n",
    "    epoch = 10\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=monmentum)\n",
    "\n",
    "    csv_dir = \"D://python_code//deep learning coursework//train.csv\"\n",
    "    data_transform = train_transform\n",
    "    train_dir = \"D://python_code//deep learning coursework//train\"\n",
    "    trainset = CellPictureDataset(train_dir, csv_file=csv_dir, transform=data_transform)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader =DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(16),\n",
    "        shuffle=True,\n",
    "        num_workers=0)\n",
    "    valloader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(16),\n",
    "        shuffle=True,\n",
    "        num_workers=0)\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        # train the model\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        # total_train = 0\n",
    "        # step_train = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # ...\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.cpu().detach().numpy()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # total_train += labels.size(0)\n",
    "            train_acc += (predicted == labels).sum().item() / len(labels)\n",
    "\n",
    "        train_loss /= len(trainloader)\n",
    "        train_acc /= len(trainloader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # validate the model\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        # total = 0\n",
    "        # correct = 0\n",
    "        val_steps = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                with torch.no_grad():\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    # total += labels.size(0)\n",
    "                    # correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    val_loss += loss.cpu().numpy()\n",
    "                    # val_loss += loss.item()\n",
    "                    val_acc += (predicted == labels).sum().item() / len(labels)\n",
    "\n",
    "        val_loss /= len(valloader)\n",
    "        val_acc /= len(valloader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    # create a figure with two subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # plot the training and validation losses\n",
    "    axs[0].plot(train_losses, label=\"Training Loss\")\n",
    "    axs[0].plot(val_losses, label=\"Validation Loss\")\n",
    "    axs[0].legend()\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "    # plot the training and validation accuracies\n",
    "    axs[1].plot(train_accs, label=\"Training Accuracy\")\n",
    "    axs[1].plot(val_accs, label=\"Validation Accuracy\")\n",
    "    axs[1].legend()\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_and_visualize(model = MyConvNet(l1=256), learning_rate=0.008830556690188944, monmentum=0.6994160916279658)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "#                                       transforms.RandomHorizontalFlip(),\n",
    "#                                       transforms.ToTensor(),\n",
    "#                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#                                      ])\n",
    "# train_dataset = CellPictureDataset('train', csv_file='train.csv', transform=data_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "# test_transform = transforms.Compose([transforms.Resize(256),\n",
    "#                                      transforms.CenterCrop(224),\n",
    "#                                      transforms.ToTensor(),\n",
    "#                                      transforms.Normalize(\n",
    "#                                          mean=[0.485, 0.456, 0.406], \n",
    "#                                          std=[0.229, 0.224, 0.225])\n",
    "#                                     ])\n",
    "# test_dataset = CellPictureDataset('train',transform=data_transform, test=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "# model = MyConvNet()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# learning_rate = 0.008830556690188944\n",
    "# momentum = 0.6994160916279658\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5780\\1002033374.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyConvNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = MyConvNet(l1=256)\n",
    "images, labels = next(iter(train_loader))\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "confusion_matrix_model = confusion_matrix(labels.numpy(), predicted.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "def cnf_matrix_plotter(cm, classes, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    传入混淆矩阵和标签名称列表，绘制混淆矩阵\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    # plt.colorbar() # 色条\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    \n",
    "    plt.title('confusion matrix', fontsize=30)\n",
    "    plt.xlabel('predicted', fontsize=25, c='r')\n",
    "    plt.ylabel('real class', fontsize=25, c='r')\n",
    "    plt.tick_params(labelsize=12) # 设置类别文字大小\n",
    "    # plt.xticks(tick_marks, classes, rotation=90) # 横轴文字旋转\n",
    "    # plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # 写数字\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                 fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # plt.savefig('混淆矩阵.pdf', dpi=300) # 保存图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_plotter(confusion_matrix_model, labels.numpy().tolist(),cmap='Blues')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doing_occulsion(model, img_pil):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    transform_A = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),          \n",
    "        transforms.ToTensor()         \n",
    "    ])\n",
    "    transform_B = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "    rc_img = transform_A(img_pil)\n",
    "    rc_img_norm = np.transpose(rc_img.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "    input_tensor = transform_B(rc_img).unsqueeze(0).to(device)\n",
    "    pred_logits = model(input_tensor)\n",
    "    pred_softmax = F.softmax(pred_logits, dim=1)\n",
    "    pred_conf, pred_id = torch.topk(pred_softmax, 1)\n",
    "    pred_conf = pred_conf.detach().cpu().numpy().squeeze().item()\n",
    "    pred_id = pred_id.detach().cpu().numpy().squeeze().item()\n",
    "    \n",
    "    occlusion = Occlusion(model)\n",
    "    attributions_occ = occlusion.attribute(input_tensor,\n",
    "                                       strides = (3, 8, 8), # 遮挡滑动移动步长\n",
    "                                       target=pred_id, # 目标类别\n",
    "                                       sliding_window_shapes=(3, 15, 15), # 遮挡滑块尺寸\n",
    "                                       baselines=0) # 被遮挡滑块覆盖的像素值\n",
    "    attributions_occ_norm = np.transpose(attributions_occ.detach().cpu().squeeze().numpy(), (1,2,0))\n",
    "    viz.visualize_image_attr_multiple(attributions_occ_norm, # 224 224 3\n",
    "                                  rc_img_norm,           # 224 224 3\n",
    "                                  [\"original_image\", \"heat_map\"],\n",
    "                                  [\"all\", \"positive\"],\n",
    "                                  show_colorbar=True,\n",
    "                                  outlier_perc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doing_ig(model, img_pil):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    transform_A = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),          \n",
    "        transforms.ToTensor()         \n",
    "    ])\n",
    "    transform_B = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    model.eval().to(device)\n",
    "    rc_img = transform_A(img_pil)\n",
    "    rc_img_norm = np.transpose(rc_img.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "    input_tensor = transform_B(rc_img).unsqueeze(0).to(device)\n",
    "    pred_logits = model(input_tensor)\n",
    "    pred_softmax = F.softmax(pred_logits, dim=1)\n",
    "    pred_conf, pred_id = torch.topk(pred_softmax, 1)\n",
    "    pred_conf = pred_conf.detach().cpu().numpy().squeeze().item()\n",
    "    pred_id = pred_id.detach().cpu().numpy().squeeze().item()\n",
    "    \n",
    "    integrated_gradients = IntegratedGradients(model)\n",
    "    attributions_ig = integrated_gradients.attribute(input_tensor, target=pred_id, n_steps=200)\n",
    "    attributions_ig_norm = np.transpose(attributions_ig.detach().cpu().squeeze().numpy(), (1,2,0))\n",
    "    plt.imshow(attributions_ig_norm[:, :, 0] * 100)\n",
    "\n",
    "    default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n",
    "# 可视化 IG 值\n",
    "    viz.visualize_image_attr(attributions_ig_norm, # 224,224,3\n",
    "                            rc_img_norm,          # 224,224,3\n",
    "                            method='heat_map',\n",
    "                            cmap=default_cmap,\n",
    "                            show_colorbar=True,\n",
    "                            sign='positive',\n",
    "                            outlier_perc=1)\n",
    "    \n",
    "    noise_tunnel = NoiseTunnel(integrated_gradients)\n",
    "\n",
    "    # 获得输入图像每个像素的 IG 值\n",
    "    attributions_ig_nt = noise_tunnel.attribute(input_tensor, nt_samples=2, nt_type='smoothgrad_sq', target=pred_id)\n",
    "\n",
    "    # 转为 224 x 224 x 3的数据维度\n",
    "    attributions_ig_nt_norm = np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "\n",
    "    # 设置配色方案\n",
    "    default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                    [(0, '#ffffff'),\n",
    "                                                    (0.25, '#000000'),\n",
    "                                                    (1, '#000000')], N=256)\n",
    "\n",
    "    viz.visualize_image_attr_multiple(attributions_ig_nt_norm, # 224 224 3\n",
    "                                    rc_img_norm, # 224 224 3\n",
    "                                    [\"original_image\", \"heat_map\"],\n",
    "                                    [\"all\", \"positive\"],\n",
    "                                    cmap=default_cmap,\n",
    "                                    show_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model1(lr, monmentum, l1):\n",
    "    # total_train_step = 0\n",
    "    # total_test_step = 0\n",
    "    model = MyConvNet(l1=l1)\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    model.to(device)\n",
    "\n",
    "    csv_dir = \"D://python_code//deep learning coursework//train.csv\"\n",
    "    # data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "    train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "    train_dir = \"D://python_code//deep learning coursework//train\"\n",
    "    trainset = CellPictureDataset(train_dir, csv_file=csv_dir, transform=train_transform)\n",
    "    train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    \n",
    "    epoch = 35\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(lr=lr, momentum=monmentum)\n",
    "    for i in range(epoch):\n",
    "        # print(\"-------no.{} train begin\".format(i+1))\n",
    "        for data in train_loader:\n",
    "            image, label = data\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            outputs = model(image)\n",
    "            loss = loss_fn(outputs, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    torch.save(model, 'myconvnet.pth')\n",
    "    return model\n",
    "\n",
    "            # total_train_step = total_train_step + 1\n",
    "            # if total_train_step % 100 == 0:\n",
    "            #     print(\"trian times: {}, Loss: {}\".format(total_train_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model2(lr, monmentum):\n",
    "\n",
    "    model = torchvision.models.resnet18()   \n",
    "    model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    model.to(device)\n",
    "    \n",
    "    csv_dir = \"D://python_code//deep learning coursework//train.csv\"\n",
    "    # data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "    train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "    train_dir = \"D://python_code//deep learning coursework//train\"\n",
    "    trainset = CellPictureDataset(train_dir, csv_file=csv_dir, transform=train_transform)\n",
    "    train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "    epoch = 35\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=monmentum)\n",
    "    for i in range(epoch):\n",
    "        print(\"-------no.{} train begin\".format(i+1))\n",
    "        for data in train_loader:\n",
    "            image, label = data\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            outputs = model(image)\n",
    "            loss = loss_fn(outputs, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    torch.save(model, 'resnet18.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(model):\n",
    "    test_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                         transforms.CenterCrop(224),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(\n",
    "                                             mean=[0.485, 0.456, 0.406], \n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "                                        ])\n",
    "    testset = CellPictureDataset(\"test\",transform=test_transform, test=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    model.eval().to(device)\n",
    "\n",
    "    labels = []\n",
    "    for data in testloader:\n",
    "            image = data.to(device)\n",
    "            outputs = model(image)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            label_list = list(predicted.cpu().numpy())\n",
    "            for label in label_list:\n",
    "                 labels.append(label)\n",
    "    data_frame = {'Filename': os.listdir('test'),\n",
    "                  'Label': labels\n",
    "                  }\n",
    "    df = pd.DataFrame(data_frame)\n",
    "    df.to_csv(\"example.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------no.1 train begin\n",
      "-------no.2 train begin\n",
      "-------no.3 train begin\n",
      "-------no.4 train begin\n",
      "-------no.5 train begin\n",
      "-------no.6 train begin\n",
      "-------no.7 train begin\n",
      "-------no.8 train begin\n",
      "-------no.9 train begin\n",
      "-------no.10 train begin\n",
      "-------no.11 train begin\n",
      "-------no.12 train begin\n",
      "-------no.13 train begin\n",
      "-------no.14 train begin\n",
      "-------no.15 train begin\n",
      "-------no.16 train begin\n",
      "-------no.17 train begin\n",
      "-------no.18 train begin\n",
      "-------no.19 train begin\n",
      "-------no.20 train begin\n",
      "-------no.21 train begin\n",
      "-------no.22 train begin\n",
      "-------no.23 train begin\n",
      "-------no.24 train begin\n",
      "-------no.25 train begin\n",
      "-------no.26 train begin\n",
      "-------no.27 train begin\n",
      "-------no.28 train begin\n",
      "-------no.29 train begin\n",
      "-------no.30 train begin\n",
      "-------no.31 train begin\n",
      "-------no.32 train begin\n",
      "-------no.33 train begin\n",
      "-------no.34 train begin\n",
      "-------no.35 train begin\n"
     ]
    }
   ],
   "source": [
    "net = get_best_model2(lr=0.00105, monmentum=0.978)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.17 GiB already allocated; 0 bytes free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5780\\823849513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg_pil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# doing_occulsion(net, img_pil)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdoing_ig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_pil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5780\\2639081694.py\u001b[0m in \u001b[0;36mdoing_ig\u001b[1;34m(model, img_pil)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mintegrated_gradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIntegratedGradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mattributions_ig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mintegrated_gradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpred_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mattributions_ig_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattributions_ig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattributions_ig_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\captum\\log\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[0;32m    284\u001b[0m             )\n\u001b[0;32m    285\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             attributions = self._attribute(\n\u001b[0m\u001b[0;32m    287\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mbaselines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;31m# scale features and compute gradients. (batch size is abbreviated as bsz)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;31m# scaled_features' dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         scaled_features_tpl = tuple(\n\u001b[0m\u001b[0;32m    329\u001b[0m             torch.cat(\n\u001b[0;32m    330\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mbaseline\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    328\u001b[0m         scaled_features_tpl = tuple(\n\u001b[0;32m    329\u001b[0m             torch.cat(\n\u001b[1;32m--> 330\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mbaseline\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m             ).requires_grad_()\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    328\u001b[0m         scaled_features_tpl = tuple(\n\u001b[0;32m    329\u001b[0m             torch.cat(\n\u001b[1;32m--> 330\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mbaseline\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m             ).requires_grad_()\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.17 GiB already allocated; 0 bytes free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "img_path = \"train/10.png\"\n",
    "img_pil = Image.open(img_path)\n",
    "# doing_occulsion(net, img_pil)\n",
    "doing_ig(net, img_pil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_result(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
