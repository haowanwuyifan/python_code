{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1.png', '2.png', '3.png', ..., '7558.png', '7559.png', '7560.png'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picture_frames = pd.read_csv('train.csv')\n",
    "picture_name = picture_frames.iloc[:,0]\n",
    "label = picture_frames.iloc[:,1]\n",
    "picture_name = np.asarray(picture_name)\n",
    "label = np.asarray(label)\n",
    "picture_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellPictureDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, csv_file=None, transform=None, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        if csv_file is not None:\n",
    "            self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        if self.transform:\n",
    "            self.image = self.transform(image)\n",
    "        if self.test is False:\n",
    "            label = self.data_frame.iloc[idx, 1]\n",
    "            # label = np.asarray(label)\n",
    "            # self.label = torch.from_numpy(label).unsqueeze_(0)\n",
    "            sample = [self.image, self.label]\n",
    "            return sample\n",
    "        else:\n",
    "            return self.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[[ 0.6941,  0.5529,  0.4275,  ...,  0.8353,  0.8745,  0.8824],\n",
      "         [ 0.5843,  0.5216,  0.4902,  ...,  0.9059,  0.9294,  0.9373],\n",
      "         [ 0.4667,  0.4902,  0.5451,  ...,  0.9765,  0.9843,  0.9843],\n",
      "         ...,\n",
      "         [ 0.8745,  0.8745,  0.8745,  ...,  0.3333,  0.2314,  0.1686],\n",
      "         [ 0.8667,  0.8745,  0.8745,  ...,  0.3725,  0.3804,  0.3098],\n",
      "         [ 0.8588,  0.8588,  0.8667,  ...,  0.3255,  0.3333,  0.2863]],\n",
      "\n",
      "        [[ 0.4353,  0.2706,  0.1451,  ...,  0.7725,  0.8118,  0.8196],\n",
      "         [ 0.3176,  0.2392,  0.2000,  ...,  0.8431,  0.8745,  0.8824],\n",
      "         [ 0.1922,  0.2000,  0.2392,  ...,  0.9137,  0.9294,  0.9294],\n",
      "         ...,\n",
      "         [ 0.8667,  0.8588,  0.8588,  ...,  0.0275, -0.0745, -0.1373],\n",
      "         [ 0.8588,  0.8588,  0.8667,  ...,  0.0588,  0.0667, -0.0039],\n",
      "         [ 0.8431,  0.8510,  0.8588,  ...,  0.0039,  0.0196, -0.0275]],\n",
      "\n",
      "        [[ 0.8588,  0.7176,  0.6000,  ...,  0.8431,  0.8745,  0.8745],\n",
      "         [ 0.7804,  0.7255,  0.6863,  ...,  0.9059,  0.9294,  0.9294],\n",
      "         [ 0.7020,  0.7176,  0.7569,  ...,  0.9843,  0.9843,  0.9843],\n",
      "         ...,\n",
      "         [ 0.9137,  0.9059,  0.8980,  ...,  0.6471,  0.5608,  0.5059],\n",
      "         [ 0.9137,  0.9059,  0.9059,  ...,  0.6706,  0.6863,  0.6235],\n",
      "         [ 0.8980,  0.9059,  0.9059,  ...,  0.6157,  0.6235,  0.5765]]]), tensor([0])]\n",
      "1 [tensor([[[ 0.8353,  0.8667,  0.9059,  ...,  0.1137,  0.8510,  1.0000],\n",
      "         [ 0.8824,  0.8980,  0.9137,  ..., -0.1373,  0.6157,  1.0000],\n",
      "         [ 0.9294,  0.9294,  0.9137,  ..., -0.3412,  0.3961,  1.0000],\n",
      "         ...,\n",
      "         [ 0.3569,  0.3098,  0.2471,  ...,  0.8667,  0.8667,  0.8745],\n",
      "         [ 0.3255,  0.3255,  0.3333,  ...,  0.8039,  0.8667,  0.9216],\n",
      "         [ 0.2235,  0.2549,  0.3176,  ...,  0.8196,  0.8824,  0.9294]],\n",
      "\n",
      "        [[ 0.7647,  0.8039,  0.8431,  ...,  0.0431,  0.8275,  0.9765],\n",
      "         [ 0.8039,  0.8275,  0.8510,  ..., -0.2235,  0.5608,  0.9765],\n",
      "         [ 0.8510,  0.8510,  0.8510,  ..., -0.4510,  0.3020,  0.9608],\n",
      "         ...,\n",
      "         [ 0.0431, -0.0118, -0.0824,  ...,  0.7961,  0.7882,  0.8039],\n",
      "         [ 0.0118,  0.0039,  0.0039,  ...,  0.7333,  0.7961,  0.8510],\n",
      "         [-0.0902, -0.0588, -0.0039,  ...,  0.7490,  0.8118,  0.8588]],\n",
      "\n",
      "        [[ 0.8667,  0.8980,  0.9216,  ...,  0.3882,  0.8980,  1.0000],\n",
      "         [ 0.9059,  0.9216,  0.9294,  ...,  0.1686,  0.7176,  1.0000],\n",
      "         [ 0.9529,  0.9373,  0.9294,  ...,  0.0039,  0.5686,  1.0000],\n",
      "         ...,\n",
      "         [ 0.6235,  0.5922,  0.5451,  ...,  0.8980,  0.8980,  0.9137],\n",
      "         [ 0.6000,  0.6078,  0.6157,  ...,  0.8353,  0.8980,  0.9451],\n",
      "         [ 0.4980,  0.5294,  0.5843,  ...,  0.8431,  0.8980,  0.9451]]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CellPictureDataset('train/', csv_file='train.csv', transform=data_transform)\n",
    "test_dataset = CellPictureDataset('train/',transform=data_transform, test=True)\n",
    "for i in range(2):\n",
    "    data = train_dataset[i]\n",
    "    print(i, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "class MyConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(collections.OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3,64,3,padding=1)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(64,64,3,padding=1)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(64,32,3,padding=1)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            # Put in a linear layers ...\n",
    "            ('flatten', nn.Flatten()),                                          \n",
    "            ('fc1', nn.Linear(32*10000,1028)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(1028,4)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyConvNet()\n",
    "data = torch.ones(10,3,100,100)\n",
    "result = model(data)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cell_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
